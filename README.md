# LLM-Detection-MentalHealth

This is our code base for a collaboration project with the Nock Lab: Quantifying Differences in LLM and Human Feedback on Mental Health
Related Surveys.

## About this Project
Our study presents a two-fold contribution to the field. Firstly, we assembled a comprehensive benchmark dataset for mental health surveys, synthesizing responses generated by Large Language Models (LLMs) with human responses obtained from our partner. To address the linguistic features between human and LLM-generated responses, we conducted a meticulous linguistic analysis. Secondly, we introduced robust detection methods for identifying AI-generated responses, including tree-based models, transformer-based models, and few-shot learning techniques. Notably, our models demonstrated an impressive test accuracy of up to 96\%. This dual-focused approach addresses both the generation and detection aspects of fake responses, providing a holistic framework for enhancing the authenticity and reliability of survey data in the context of mental health research.

## Requirements
Codes are contained in `./notebook directory`. Simply run the ``ipynb`` notebooks to install the requirements. 
## License
This project is licensed under the MIT License. Feel free to use, modify, and distribute the code according to the terms specified in the license.
